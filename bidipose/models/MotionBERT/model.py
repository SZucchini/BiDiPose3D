"""MotionBERT model scripts are copy of repository originally released under the Apache-2.0 License.
Details of the original repository is as follows:
- Original repository: https://github.com/Walter0807/MotionBERT
"""

import math
import warnings
from collections import OrderedDict

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from bidipose.models.base import BaseModel

from .drop import DropPath


def _broadcast_temporal(cam_params, pose_frames):
    bs, num_tokens, dim = cam_params.shape
    cam_params = cam_params.unsqueeze(1).expand(bs, pose_frames, num_tokens, dim)
    return cam_params.reshape(bs * pose_frames, num_tokens, dim)


def _broadcast_spatial(cam_params, num_joints):
    bs, num_tokens, dim = cam_params.shape
    cam_params = cam_params.unsqueeze(2).expand(bs, num_tokens, num_joints, dim)
    return cam_params


class SinusoidalPositionEmbeddings(nn.Module):
    """Sinusoidal position embeddings for timestep encoding."""

    def __init__(self, dim: int):
        """Initialize sinusoidal position embeddings.

        Args:
            dim (int): Embedding dimension.

        """
        super().__init__()
        self.dim = dim

    def forward(self, time: torch.Tensor) -> torch.Tensor:
        """Forward pass for sinusoidal position embeddings.

        Args:
            time (torch.Tensor): Timestep tensor with shape (B,).

        Returns:
            torch.Tensor: Sinusoidal embeddings with shape (B, dim).

        """
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        lower_bound_cdf = norm_cdf((a - mean) / std)
        upper_bound_cdf = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [lower_bound_cdf, upper_bound_cdf], then translate to
        # [2*lower_bound_cdf-1, 2*upper_bound_cdf-1].
        tensor.uniform_(2 * lower_bound_cdf - 1, 2 * upper_bound_cdf - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)

    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class MLP(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0,
        st_mode="vanilla",
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim**-0.5

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.mode = st_mode
        if self.mode == "parallel":
            self.ts_attn = nn.Linear(dim * 2, dim * 2)
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        else:
            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj_drop = nn.Dropout(proj_drop)

        self.attn_count_s = None
        self.attn_count_t = None

    def forward(self, x, seqlen=1):
        B, N, C = x.shape

        if self.mode == "series":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == "parallel":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x_t = self.forward_temporal(q, k, v, seqlen=seqlen)
            x_s = self.forward_spatial(q, k, v)

            alpha = torch.cat([x_s, x_t], dim=-1)
            alpha = alpha.mean(dim=1, keepdim=True)
            alpha = self.ts_attn(alpha).reshape(B, 1, C, 2)
            alpha = alpha.softmax(dim=-1)
            x = x_t * alpha[:, :, :, 1] + x_s * alpha[:, :, :, 0]
        elif self.mode == "coupling":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_coupling(q, k, v, seqlen=seqlen)
        elif self.mode == "vanilla":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
        elif self.mode == "temporal":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_temporal(q, k, v, seqlen=seqlen)
        elif self.mode == "spatial":
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = (
                qkv[0],
                qkv[1],
                qkv[2],
            )  # make torchscript happy (cannot use tensor as tuple)
            x = self.forward_spatial(q, k, v)
        else:
            raise NotImplementedError(self.mode)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def reshape_T(self, x, seqlen=1, inverse=False):
        if not inverse:
            N, C = x.shape[-2:]
            x = x.reshape(-1, seqlen, self.num_heads, N, C).transpose(1, 2)
            x = x.reshape(-1, self.num_heads, seqlen * N, C)  # (B, H, TN, c)
        else:
            TN, C = x.shape[-2:]
            x = x.reshape(-1, self.num_heads, seqlen, TN // seqlen, C).transpose(1, 2)
            x = x.reshape(-1, self.num_heads, TN // seqlen, C)  # (BT, H, N, C)
        return x

    def forward_coupling(self, q, k, v, seqlen=8):
        BT, _, N, C = q.shape
        q = self.reshape_T(q, seqlen)
        k = self.reshape_T(k, seqlen)
        v = self.reshape_T(v, seqlen)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = self.reshape_T(x, seqlen, inverse=True)
        x = x.transpose(1, 2).reshape(BT, N, C * self.num_heads)
        return x

    def forward_spatial(self, q, k, v):
        B, _, N, C = q.shape
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = x.transpose(1, 2).reshape(B, N, C * self.num_heads)
        return x

    def forward_temporal(self, q, k, v, seqlen=8):
        B, _, N, C = q.shape
        qt = q.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4)  # (B, H, N, T, C)
        kt = k.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4)  # (B, H, N, T, C)
        vt = v.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4)  # (B, H, N, T, C)

        attn = (qt @ kt.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ vt  # (B, H, N, T, C)
        x = x.permute(0, 3, 2, 1, 4).reshape(B, N, C * self.num_heads)
        return x

    def count_attn(self, attn):
        attn = attn.detach().cpu().numpy()
        attn = attn.mean(axis=1)
        attn_t = attn[:, :, 1].mean(axis=1)
        attn_s = attn[:, :, 0].mean(axis=1)
        if self.attn_count_s is None:
            self.attn_count_s = attn_s
            self.attn_count_t = attn_t
        else:
            self.attn_count_s = np.concatenate([self.attn_count_s, attn_s], axis=0)
            self.attn_count_t = np.concatenate([self.attn_count_t, attn_t], axis=0)


class Block(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        mlp_out_ratio=1.0,
        qkv_bias=True,
        qk_scale=None,
        drop=0.0,
        attn_drop=0.0,
        drop_path=0.0,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        st_mode="stage_st",
        att_fuse=False,
    ):
        super().__init__()
        # assert 'stage' in st_mode
        self.st_mode = st_mode
        self.norm1_s = norm_layer(dim)
        self.norm1_t = norm_layer(dim)
        self.attn_s = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
            st_mode="spatial",
        )
        self.attn_t = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
            st_mode="temporal",
        )

        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2_s = norm_layer(dim)
        self.norm2_t = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        mlp_out_dim = int(dim * mlp_out_ratio)
        self.mlp_s = MLP(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            out_features=mlp_out_dim,
            act_layer=act_layer,
            drop=drop,
        )
        self.mlp_t = MLP(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            out_features=mlp_out_dim,
            act_layer=act_layer,
            drop=drop,
        )
        self.att_fuse = att_fuse
        if self.att_fuse:
            self.ts_attn = nn.Linear(dim * 2, dim * 2)

    def forward(self, x, cam_params):
        if self.st_mode == "stage_st":
            bst, num_joints, _ = x.shape
            bs, num_tokens, _ = cam_params.shape
            pose_frames = bst // bs
            seq_len = num_tokens + pose_frames

            cam_params_temporal = _broadcast_temporal(cam_params, pose_frames)  # (B*T, num_tokens, dim)
            x = torch.cat([cam_params_temporal, x], dim=1)  # (B*T, num_tokens+J, dim)
            x = x + self.drop_path(self.attn_s(self.norm1_s(x)))
            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))
            cam_params_temporal, x = x[:, :num_tokens, :], x[:, num_tokens:, :]

            x = x.reshape(bs, pose_frames, num_joints, -1)  # (B, T, J, D)
            new_dim = x.shape[-1]
            cam_params = cam_params_temporal.reshape(bs, -1, num_tokens, new_dim).mean(dim=1)  # (B, num_tokens, D)
            cam_params_spatial = _broadcast_spatial(cam_params, num_joints)  # (B, num_tokens, J, D)
            x = torch.cat([cam_params_spatial, x], dim=1)  # (B, num_tokens+T, J, D)
            x = x.reshape(-1, num_joints, new_dim)
            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seq_len))
            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))
            x = x.reshape(bs, -1, num_joints, new_dim)  # (B, num_tokens+T, J, D)
            cam_params_spatial, x = x[:, :num_tokens, :, :], x[:, num_tokens:, :, :]
            cam_params = cam_params_spatial.mean(dim=2)  # (B, num_tokens, D)
            x = x.reshape(bst, num_joints, -1)

        elif self.st_mode == "stage_ts":
            bst, num_joints, dim = x.shape
            bs, num_tokens, _ = cam_params.shape
            pose_frames = bst // bs
            seq_len = num_tokens + pose_frames

            x = x.reshape(bs, pose_frames, num_joints, -1)  # (B, T, J, D)
            cam_params_spatial = _broadcast_spatial(cam_params, num_joints)  # (B, num_tokens, J, D)
            x = torch.cat([cam_params_spatial, x], dim=1)  # (B, num_tokens+T, J, D)
            x = x.reshape(-1, num_joints, dim)  # (B*(num_tokens+T), J, D)
            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seq_len))
            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))
            x = x.reshape(bs, num_tokens + pose_frames, num_joints, -1)  # (B, num_tokens+T, J, D)
            cam_params_spatial, x = x[:, :num_tokens, :, :], x[:, num_tokens:, :, :]

            cam_params = cam_params_spatial.mean(dim=2)  # (B, num_tokens, D)
            cam_params_temporal = _broadcast_temporal(cam_params, pose_frames)  # (B*T, num_tokens, D)
            x = x.reshape(bs * pose_frames, num_joints, -1)  # (B*T, J, D)
            x = torch.cat([cam_params_temporal, x], dim=1)  # (B*T, num_tokens+J, D)
            x = x + self.drop_path(self.attn_s(self.norm1_s(x)))
            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))
            cam_params_temporal, x = x[:, :num_tokens, :], x[:, num_tokens:, :]
            cam_params = cam_params_temporal.reshape(bs, pose_frames, num_tokens, -1).mean(dim=1)  # (B, num_tokens, D)
        else:
            raise NotImplementedError(self.st_mode)
        return x, cam_params


class DSTformer(BaseModel):
    def __init__(
        self,
        dim_in=6,
        dim_out=6,
        dim_feat=256,
        dim_rep=512,
        depth=5,
        num_heads=8,
        mlp_ratio=4,
        num_joints=17,
        pose_frames=81,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        norm_layer=nn.LayerNorm,
        att_fuse=True,
        timestep_embed_dim=64,
        num_tokens=7,
    ):
        super().__init__()
        norm_layer = getattr(nn, norm_layer) if isinstance(norm_layer, str) else norm_layer
        self.dim_out = dim_out
        self.dim_feat = dim_feat
        self.num_tokens = num_tokens
        self.pose_frames = pose_frames
        self.num_joints = num_joints
        self.total_frames = pose_frames + num_tokens
        self.total_joints = num_joints + num_tokens
        self.joints_embed = nn.Linear(dim_in, dim_feat)
        self.cam_params_embed = nn.Linear(7, dim_feat * num_tokens)

        self.quat_embed = nn.Linear(1, dim_feat)
        self.trans_embed = nn.Linear(1, dim_feat)
        self.type_embed = nn.Parameter(torch.zeros(2, 1, dim_feat))
        trunc_normal_(self.type_embed, std=0.02)

        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks_st = nn.ModuleList(
            [
                Block(
                    dim=dim_feat,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    norm_layer=norm_layer,
                    st_mode="stage_st",
                )
                for i in range(depth)
            ]
        )
        self.blocks_ts = nn.ModuleList(
            [
                Block(
                    dim=dim_feat,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    norm_layer=norm_layer,
                    st_mode="stage_ts",
                )
                for i in range(depth)
            ]
        )
        self.norm = norm_layer(dim_feat)
        if dim_rep:
            self.pre_logits = nn.Sequential(OrderedDict([("fc", nn.Linear(dim_feat, dim_rep)), ("act", nn.Tanh())]))
        else:
            self.pre_logits = nn.Identity()
        self.head = nn.Linear(dim_rep, dim_out) if dim_out > 0 else nn.Identity()
        self.head_cam = nn.Linear(dim_rep, 7)

        self.quat_linear = nn.Linear(dim_out, 1)
        self.trans_linear = nn.Linear(dim_out, 1)

        self.temp_embed = nn.Parameter(torch.zeros(1, pose_frames, 1, dim_feat))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat))

        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(timestep_embed_dim),
            nn.Linear(timestep_embed_dim, timestep_embed_dim * 2),
            nn.GELU(),
            nn.Linear(timestep_embed_dim * 2, dim_feat),
        )

        trunc_normal_(self.temp_embed, std=0.02)
        trunc_normal_(self.pos_embed, std=0.02)
        self.apply(self._init_weights)
        self.att_fuse = att_fuse
        if self.att_fuse:
            self.ts_attn = nn.ModuleList([nn.Linear(dim_feat * 2, 2) for i in range(depth)])
            for i in range(depth):
                self.ts_attn[i].weight.data.fill_(0)
                self.ts_attn[i].bias.data.fill_(0.5)

            self.ts_attn_cam = nn.ModuleList([nn.Linear(dim_feat * 2, 2) for i in range(depth)])
            for i in range(depth):
                self.ts_attn_cam[i].weight.data.fill_(0)
                self.ts_attn_cam[i].bias.data.fill_(0.5)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_classifier(self):
        return self.head

    def reset_classifier(self, dim_out, global_pool=""):
        self.dim_out = dim_out
        self.head = nn.Linear(self.dim_feat, dim_out) if dim_out > 0 else nn.Identity()

    def forward(
        self,
        x: torch.Tensor,
        quat: torch.Tensor,
        trans: torch.Tensor,
        t: torch.Tensor | None = None,
        return_rep: bool = False,
    ):
        """Forward pass of the MotionAGFormer model.

        Args:
            x (torch.Tensor): Input 2D from two-views (B, T, J, C=3*2).
            quat (torch.Tensor): Quaternions for cam1 to cam2 (B, 4).
            trans (torch.Tensor): Translation vector for cam1 to cam2 (B, 3).
            t (torch.Tensor): Time step embedding.
            return_rep (bool): If True, returns the representation logits instead of the final output.

        Returns:
            pred_pose (torch.Tensor): Predicted 2D poses from 2 views (B, T, J, 3*2).
            pred_quat (torch.Tensor): Predicted quaternion (B, 4).
            pred_trans (torch.Tensor): Predicted translation (B, 3).

        """
        cam_params = torch.cat((quat, trans), dim=-1)  # (B, 7)
        cam_params = self.cam_params_embed(cam_params)  # (B, D * num_tokens)
        cam_params = cam_params.reshape(-1, self.num_tokens, self.dim_feat)
        cam_params = cam_params + self.type_embed[1]  # (B, num_tokens, D)

        bs, frames, joints, _ = x.shape
        x = self.joints_embed(x)
        x = x + self.pos_embed + self.type_embed[0][None, :, :]  # (B, T+7, J, D)

        if t is not None:
            time_embed = self.time_mlp(t)  # (B, D)
            cam_params = cam_params + time_embed.unsqueeze(1).expand(bs, self.num_tokens, -1)  # (B, 7, D)
            time_embed = time_embed.unsqueeze(1).unsqueeze(1)  # (B, 1, 1, D)
            time_embed = time_embed.expand(bs, x.size(1), x.size(2), -1)  # (B, T+7, J, D)
            x = x + time_embed

        x = x + self.temp_embed
        x = x.reshape(bs * self.pose_frames, joints, self.dim_feat)
        x = self.pos_drop(x)

        for idx, (blk_st, blk_ts) in enumerate(zip(self.blocks_st, self.blocks_ts, strict=False)):
            x_st, cam_params_st = blk_st(x, cam_params)
            x_ts, cam_params_ts = blk_ts(x, cam_params)
            if self.att_fuse:
                alpha = self.ts_attn[idx](torch.cat([x_st, x_ts], dim=-1)).softmax(dim=-1)
                x = x_st * alpha[..., 0:1] + x_ts * alpha[..., 1:2]
                alpha_cam = self.ts_attn_cam[idx](torch.cat([cam_params_st, cam_params_ts], dim=-1)).softmax(dim=-1)
                cam_params = cam_params_st * alpha_cam[..., 0:1] + cam_params_ts * alpha_cam[..., 1:2]
            else:
                x = 0.5 * (x_st + x_ts)
                cam_params = 0.5 * (cam_params_st + cam_params_ts)  # (B, num_tokens, D)

        x = self.norm(x).reshape(bs, frames, joints, -1)
        rep = self.pre_logits(x)
        if return_rep:
            return rep
        pred_pose = self.head(rep)

        rep_cam = self.pre_logits(cam_params.mean(1))
        cam_params = self.head_cam(rep_cam)  # (B, 7)
        pred_quat = cam_params[:, :4]
        pred_trans = cam_params[:, 4:]

        pred_quat = F.normalize(pred_quat, dim=1)
        neg_mask = pred_quat[..., 0] < 0
        pred_quat[neg_mask] = -pred_quat[neg_mask]
        pred_trans = F.normalize(pred_trans, dim=1)
        return pred_pose, pred_quat, pred_trans

    def get_representation(self, x):
        return self.forward(x, return_rep=True)
